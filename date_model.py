# -*- coding: utf-8 -*-
"""date_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Za3IvHXaGaxJ5YoeFH6jrWbFgo8vhYAO
"""

# from google.colab import drive
# drive.mount('/content/drive',force_remount=True)

# !pip install dateparser
# !pip install tqdm

import pandas as pd
import numpy as np
import dateparser
import os
from nltk.util import ngrams
from tqdm.notebook import tqdm
import re
from bs4 import BeautifulSoup
import email
import datetime

import random

# %cd '/content/drive/MyDrive/csc482/FinalProject/'

files = os.listdir('conf_emails_numbered')
files.sort()

df_annot = pd.read_csv('combined_annotations.csv')
df_annot.head()

df_annot['Event Name'].apply(lambda x : str(x).lower().find('conf') == -1).value_counts()

"""#regex dates

##Intial exploration and testing
"""

with open('conf_emails_numbered/id002.eml', 'rb') as fhdl:
  raw_email = fhdl.read()

soup = BeautifulSoup(raw_email, 'html')

email_soup = soup.find_all('foaad')[3]

for idx,line in enumerate(email_soup.text.lower().split('\n')):
    match = re.findall(".+ .+ 20\d\d",line)
    if match:
        for m in match:
            date = dateparser.parse(' '.join(m.split()[-3:]))
            if date:
                print(idx,m,date)

"""##Implementation"""

def get_dates(path):
    char_nums = []
    text_bubbles = []
    dates = []
    with open(path, 'rb') as fhdl:
        raw_email = fhdl.read()
    msg = email.message_from_bytes(raw_email)
    email_body=''
    for part in msg.walk():
        if part.get_content_type().find('text') != -1:
            email_body = part.get_payload(None, True)
    soup = BeautifulSoup(email_body, 'html')
    text = soup.text.lower()
    match = re.finditer("\d+/\d+/\d",text)
    for m in match:
        date_str = text[m.start():m.end()]
        datet = dateparser.parse(date_str,settings={'STRICT_PARSING': True})
        if datet:
            char_nums.append(m.start())
            start=max(m.start()-40,0)
            end = min(m.end()+15,len(text))
            text_bubbles.append(re.sub('\s+',' ',text[start:end]))
            dates.append(datet.replace(tzinfo=None))
    match = re.finditer("jan|feb|mar|apr|may|jun|jul|aug|sep|nov|dec",text)
    for m in match:
        date_str = text[m.start()-10:m.end()+15]
        # print(date_str)
        date_str = re.sub('-\s*\d+|,|\s+',' ',date_str)
        toks = re.split('\s+',date_str)
        # print(toks)
        datet=None
        for i in range(len(toks)-3):
            # print('reading:',' '.join(toks[i:i+3]))
            datet = dateparser.parse(' '.join(toks[i:i+3]),settings={'STRICT_PARSING': True})
            part_date = None
            if datet:
                break
            else:
                datet = dateparser.parse(' '.join(toks[i:i+2]))
        if datet:
            # print('found:',datet)
            char_nums.append(m.start())
            start=max(m.start()-40,0)
            end = min(m.end()+15,len(text))
            text_bubbles.append(re.sub('\s+',' ',text[start:end]))
            dates.append(datet.replace(tzinfo=None))
    return char_nums,text_bubbles,dates

df_all_labeled = pd.DataFrame(columns=['char_num', 'text', 'date', 'date_type'])
batch_size=100
i=0
for batch in range(int(len(files)/batch_size)):
    print('batch {}/{}'.format(batch+1,int(len(files)/batch_size)))
    df_batch = pd.DataFrame(columns=['char_num', 'text', 'date', 'date_type'])
    for filename in tqdm(files[batch*batch_size:(batch+1)*batch_size]):
        if i in [314,486,512]:
            i+=1
            continue
        char_nums,line_texts,dates = get_dates('conf_emails_numbered/'+filename)
        # print(filename)
        # create feature set
        df_date = pd.DataFrame()
        df_date['char_num'] = char_nums
        df_date['text'] = [re.sub('[^\w| ]|\d','',t) for t in line_texts]
        df_date['date'] = dates
        df_date['ID'] = [i]*len(dates)
        # create annotated set
        df_labels = pd.DataFrame()
        labels = df_annot[df_annot['ID']==i]
        if len(labels)==0:
            i+=1
            continue
        true_dates = labels[['Date (Month Day, Year "January 17, 2021")','Submission deadline', 'Notification deadline']].iloc[0]
        true_dates_dt = []
        for d in true_dates.values:
            if d is np.nan:
                true_dates_dt.append(d)
            else:
                true_dates_dt.append(dateparser.parse(d))
        df_labels['date'] = true_dates_dt
        df_labels['date_type'] = ['conf_date','sub_date','notif_date']
        df_labels.dropna(inplace=True)
        if len(df_labels)==0:
            i+=1
            continue
        if len(df_date) == 0:
            print(i)
            i+=1
            continue
        # print(df_labels)
        # print(df_date)
        # merge the two sets on date
        # df_date.sort_values('date',inplace=True)
        # df_labels.sort_values('date',inplace=True)
        df_labeled = pd.merge(df_date,df_labels, on='date')
        # df_labeled.drop_duplicates(subset=['text','date','date_type'],inplace=True)
        df_labeled.fillna('none',inplace=True)
        # print(df_labeled)

        df_batch = df_batch.append(df_labeled)
        i+=1
    df_all_labeled = df_all_labeled.append(df_batch,ignore_index=True)

"""##Word Cloud"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

df_all_labeled['date_type'].unique()

combined_text=''
for text in df_all_labeled[df_all_labeled['date_type']=='sub_date']['text']:
    combined_text += re.sub('\s+',' ',text) + ' '

pd.Series(re.split('\s+',combined_text)).value_counts()

word_count={}
for t in combined_text.split():
    if t in word_count:
        word_count[t]+=1
    else:
        word_count[t]=1

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = set(STOPWORDS),
                min_font_size = 10).generate_from_frequencies(word_count)
plt.figure(figsize = (5, 5), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

"""##ml"""

# pip install XGBoost

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

df_all_labeled.drop_duplicates(inplace=True)
df_all_labeled

tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english',min_df=2)
tfidf_wm = tfidfvectorizer.fit_transform(df_all_labeled['text'])
tfidf_tokens = tfidfvectorizer.get_feature_names()
df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),columns = tfidf_tokens)
df_tfidfvect['ID'] = df_all_labeled['ID']
df_tfidfvect['date_type'] = df_all_labeled['date_type']
df_tfidfvect

X_train, X_test, y_train, y_test = train_test_split(df_tfidfvect.drop(axis=0,columns=['ID','date_type']), df_tfidfvect['date_type'], test_size=0.20,random_state=2)

xgb_classifier = xgb.XGBClassifier(learning_rate = 0.1, max_depth = 5, alpha = 10, n_estimators = 10)
xgb_classifier.fit(X_train,y_train)
y_pred = xgb_classifier.predict(X_test)

y_pred = xgb_classifier.predict(X_test)

confusion_matrix(y_test, y_pred)

from sklearn.metrics import f1_score

f1_score(y_test,y_pred,average='micro')

import pickle 

pickle.dump(xgb_classifier, open('xgb_classifier2.pkl', "wb"))

xgb_model_loaded = pickle.load(open('xgb_classifier2.pkl', "rb"))

pickle.dump(tfidfvectorizer, open('tfidfvec2.pkl', "wb"))