# -*- coding: utf-8 -*-
"""Event_Name_Extractor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ByEqqvm0pEzjOBffbuTBF1qxpxI2wmsA
"""

# from google.colab import drive
# drive.mount('/content/drive')

# !pip install email
# !pip install catboost
# !pip install python-Levenshtein

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/csc482/FinalProject

# !ls

import re
import pandas as pd
from Levenshtein import ratio as levenshtein_ratio
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer

def create_training_data(df_annots):
  training_data = []
  for i in df_annots.index:
    row = df_annots.loc[i]
    if pd.isnull(row['email_text']) or pd.isnull(row['Event Name']):
      continue
    text = row['email_text'].replace('\n', ' ').replace('\t', ' ')
    event_name = row['Event Name']
    if event_name not in text:
      print(event_name, row['ID'])
      continue
    event_name = event_name.strip()
    start = text.index(event_name)
    end = start + len(event_name)
    spacy_row = (text, [(start, end, "EVENT")])
    training_data.append(spacy_row)
  return training_data

"""Initial Clean of Data"""
df_annots = pd.read_csv('reannotated_emails.csv')
print(len(df_annots))
df_annots = df_annots.dropna(subset=['Event Name'])
print(len(df_annots))
df_annots['email_text'] = df_annots['email_text'].str.replace("(?<=[a-z])(?=[A-Z0-9])", " ", regex=True).str.replace("[\{\(\[].*?[\)\]\}]", "", regex=True).str.replace('\n', ' ').str.replace('\t', ' ').str.replace(' +', ' ', regex=True).str.replace('Conf.', 'Conf').str.replace('Symp.', 'Symp').str.strip()
df_annots['Event Name'] = df_annots['Event Name'].str.replace("(?<=[a-z])(?=[A-Z0-9])", " ", regex=True).str.replace("[\{\(\[].*?[\)\]\}]", "", regex=True).str.replace('\n', ' ').str.replace('\t', ' ').str.replace(' +', ' ', regex=True).str.replace('Conf.', 'Conf').str.replace('Symp.', 'Symp').str.strip()
all_data = create_training_data(df_annots)

"""Lemmatize, tokenize, and label sentences"""
lemmatizer = WordNetLemmatizer()
all_sents = []
for data in all_data:
  text = data[0]
  name = text[data[1][0][0]:data[1][0][1]]
  sents = sent_tokenize(text)
  for sent in sents:
    if name in sent:
      name_in_sent = 1
    else:
      name_in_sent = 0
    words = [lemmatizer.lemmatize(w) for w in sent.split()]
    sent = " ".join(words)
    all_sents.append((sent, name_in_sent))
df_all_sents = pd.DataFrame(all_sents, columns=['sent', 'name_in_sent'])
df_all_sents

"""Create tfidf vector matrix"""
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=2)
tfidf_wm = vectorizer.fit_transform(df_all_sents['sent'].to_list())
columns = vectorizer.get_feature_names_out()
stop_i = 0
for i in range(len(columns)):
  colname = columns[i]
  if colname[0] == 'a':
    stop_i = i
    break

df_sent_vecs = pd.DataFrame(tfidf_wm.toarray(), columns=columns).iloc[:, stop_i:]
df_sent_vecs

"""Train test split"""
from sklearn.model_selection import train_test_split
X = df_sent_vecs
y = df_all_sents['name_in_sent']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15)
len(X_train), len(X_val), len(X_test)

"""Train the catboost classifier"""
from catboost import CatBoostClassifier
clf = CatBoostClassifier(
    iterations=100,
    random_seed=42,
    learning_rate=0.50,
    custom_loss=['AUC', 'Accuracy']
)
clf.fit(
    X_train, y_train,
    eval_set=(X_val, y_val),
)

"""Score the classifier"""
from sklearn.metrics import precision_recall_fscore_support
y_pred = clf.predict(X_test)
print("\n Precision, Recall, F-score, Support for sentence classifier:")
print(precision_recall_fscore_support(y_test, y_pred, average='macro'))

"""Gets the list of potenial names for a given sentence"""
stop_words = stopwords.words('english')
def get_consecutive_words(sent):
  months = {'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december'}
  sent = sent.replace('-', '')
  sent = sent.replace('/', ' ')
  words = nltk.word_tokenize(sent)
  phrases = []
  current_phrase = []
  for word in words:
    if bool(re.match("[A-Z]+'\d+", word)):
      if len(current_phrase) > 0:
          # remove extra stopwords
          i = len(current_phrase) - 1
          while (current_phrase[i] in stop_words):
            del current_phrase[i]
            i -= 1
          phrases.append(' '.join(current_phrase))
          current_phrase = []
      phrases.append(word)
    else:
      if (bool(re.match(r'\w*[A-Z]\w*', word)) or bool(re.search(r'\d', word))) and word.lower() not in months:
        current_phrase.append(word)
      else:
        if word in stop_words and len(current_phrase) > 0:
          current_phrase.append(word)
        else:
          if len(current_phrase) > 0:
            # remove extra stopwords
            i = len(current_phrase) - 1
            while (current_phrase[i] in stop_words):
              del current_phrase[i]
              i -= 1
            phrases.append(' '.join(current_phrase))
            current_phrase = []
  better_phrases = []
  for phrase in phrases:
    if len(phrase.split()) > 1:
      better_phrases.append(phrase)
  if len(better_phrases) == 0:
    return phrases
  return better_phrases


"""Selects the most likely potential name"""
def select_conference(phrases):
  if len(phrases) == 0:
    return ""
  for phrase in phrases:
    if 'conference' in phrase.lower() or 'confrence' in phrase.lower():
      return phrase
  for phrase in phrases:
    if 'conf' in phrase.lower():
      return phrase
  for phrase in phrases:
    if 'event' in phrase.lower():
      return phrase
  for phrase in phrases:
    if 'congress' in phrase.lower():
      return phrase
  return max(phrases, key=len)


"""Run the whole model on all data"""
df_results = pd.DataFrame()
i = 0
for text, annotations in all_data:
  name = text[annotations[0][0]:annotations[0][1]]
  sents = sent_tokenize(text)
  lemmatized_sents = []
  for sent in sents:
    words = [lemmatizer.lemmatize(w) for w in sent.split()]
    sent = " ".join(words)
    lemmatized_sents.append(sent)
  sents_wm = vectorizer.transform(lemmatized_sents)
  df_sents_vecs = pd.DataFrame(sents_wm.toarray(), columns=columns).iloc[:, stop_i:]
  predictions = clf.predict_proba(df_sents_vecs)
  df_predictions = pd.DataFrame(predictions, columns=['not_in_sent', 'in_sent'])
  df_predictions['compound'] = df_predictions['in_sent'] - df_predictions['not_in_sent']
  best = list(df_predictions['compound'].sort_values(ascending=False).index)[0]
  best_sent = sents[best]
  if name in best_sent:
    name_in_best_sent = True
  else:
    name_in_best_sent = False
  pred_name = select_conference(get_consecutive_words(best_sent))
  df_results = df_results.append({'pred_name': pred_name, 'actual_name': name, 'name_in_best_sent': name_in_best_sent}, ignore_index=True)
  i += 1
df_results


"""Get the levenshtein ratio for each prediction"""
df_results['ratio'] = 0
for index, row in df_results.iterrows():
  ratio = levenshtein_ratio(row['actual_name'], row['pred_name'])
  df_results.loc[index, 'ratio'] = ratio
df_results


"""Average levenshtein ratio -- effectively the accuracy"""
print("\nFinal average levenshtein ratio score:")
print(df_results['ratio'].mean())
